# **üìå Gu√≠a de Pub/Sub en Google Cloud**  

## **1Ô∏è‚É£ Introducci√≥n a Pub/Sub**  

### **üîπ ¬øQu√© es Pub/Sub?**  
Pub/Sub (**Publish-Subscribe**) es un **servicio de mensajer√≠a as√≠ncrono y escalable** de Google Cloud, dise√±ado para la comunicaci√≥n entre sistemas distribuidos.  

‚úî **Desacopla productores y consumidores** ‚Üí Permite comunicaci√≥n as√≠ncrona sin dependencia directa.  
‚úî **Alta disponibilidad y escalabilidad** ‚Üí Gestionado por Google, sin necesidad de administrar infraestructura.  
‚úî **Procesamiento en tiempo real** ‚Üí Compatible con **Dataflow, BigQuery, Cloud Functions, Cloud Run, etc.**  
‚úî **Entrega confiable** ‚Üí Soporta reintentos autom√°ticos y pol√≠ticas de reintento.  

---

## **2Ô∏è‚É£ Arquitectura de Mensajer√≠a en Pub/Sub**  
Pub/Sub sigue un modelo **publish-subscribe**, donde los **productores** (publishers) env√≠an mensajes a un **t√≥pico**, y los **consumidores** (subscribers) reciben esos mensajes a trav√©s de **suscripciones**.  

üìå **Componentes principales:**  
1. **T√≥pico** ‚Üí Punto central donde los publishers env√≠an mensajes.  
2. **Suscripci√≥n** ‚Üí Canal mediante el cual los suscriptores reciben mensajes.  
3. **Publisher** ‚Üí Aplicaci√≥n que publica mensajes en un t√≥pico.  
4. **Subscriber** ‚Üí Aplicaci√≥n que recibe mensajes de una suscripci√≥n.  
5. **Mensaje** ‚Üí Unidad de datos transmitida.  

### **üîπ Flujo de Datos en Pub/Sub**  
```plaintext
Publisher ‚Üí (env√≠a mensaje) ‚Üí T√≥pico ‚Üí (entrega a) ‚Üí Suscripci√≥n ‚Üí (procesa) ‚Üí Subscriber
```

üìå **Ejemplo de arquitectura:**  
- Un servicio de pagos publica eventos en un t√≥pico.  
- Dataflow procesa estos eventos en tiempo real.  
- BigQuery almacena los datos para an√°lisis.  

---

## **3Ô∏è‚É£ Creaci√≥n de un Pub/Sub en Google Cloud**  

### **üîπ Crear un T√≥pico Pub/Sub**  
```bash
gcloud pubsub topics create mi-topico
```

### **üîπ Crear una Suscripci√≥n**  
```bash
gcloud pubsub subscriptions create mi-suscripcion --topic=mi-topico
```

### **üîπ Publicar un Mensaje en un T√≥pico**  
```bash
gcloud pubsub topics publish mi-topico --message="Hola, Pub/Sub"
```

### **üîπ Leer Mensajes desde una Suscripci√≥n**  
```bash
gcloud pubsub subscriptions pull mi-suscripcion --auto-ack
```

---

## **4Ô∏è‚É£ Tipos de Suscripci√≥n en Pub/Sub**  

### **üîπ Suscripci√≥n PULL**  
‚úî **El suscriptor recupera manualmente los mensajes.**  
‚úî √ötil cuando se necesita control sobre el procesamiento.  
‚úî Implementaci√≥n con la librer√≠a de Python:  

```python
from google.cloud import pubsub_v1

subscriber = pubsub_v1.SubscriberClient()
subscription_path = subscriber.subscription_path("mi-proyecto", "mi-suscripcion")

def callback(message):
    print(f"Mensaje recibido: {message.data}")
    message.ack()  # Confirmar recepci√≥n

subscriber.subscribe(subscription_path, callback=callback)
print("Escuchando mensajes...")
```

### **üîπ Suscripci√≥n PUSH**  
‚úî **Pub/Sub env√≠a los mensajes autom√°ticamente a un endpoint HTTP.**  
‚úî √ötil para integraci√≥n con microservicios o APIs.  

```bash
gcloud pubsub subscriptions create mi-suscripcion-push \
  --topic=mi-topico \
  --push-endpoint=https://mi-api.com/mensaje
```

---

## **5Ô∏è‚É£ Integraci√≥n de Pub/Sub con Dataflow (Apache Beam)**  
Pub/Sub se integra con **Dataflow** para procesar eventos en **tiempo real**.  

### **üîπ Ejemplo de un Pipeline en Dataflow con Pub/Sub**  
üìå **Este pipeline lee datos de Pub/Sub y los escribe en BigQuery.**  

```python
import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions

PROJECT_ID = "mi-proyecto"
TOPIC = f"projects/{PROJECT_ID}/topics/mi-topico"
TABLE = f"{PROJECT_ID}:dataset.tabla"

pipeline_options = PipelineOptions(streaming=True, project=PROJECT_ID)

with beam.Pipeline(options=pipeline_options) as p:
    (
        p
        | "Leer desde Pub/Sub" >> beam.io.ReadFromPubSub(topic=TOPIC)
        | "Decodificar" >> beam.Map(lambda x: x.decode("utf-8"))
        | "Transformar en JSON" >> beam.Map(lambda x: {"mensaje": x})
        | "Escribir en BigQuery" >> beam.io.WriteToBigQuery(TABLE)
    )
```

‚úî **`ReadFromPubSub(topic=TOPIC)`** ‚Üí Lee datos en tiempo real desde Pub/Sub.  
‚úî **`WriteToBigQuery(TABLE)`** ‚Üí Inserta los datos en BigQuery.  

---

## **6Ô∏è‚É£ Integraci√≥n de Pub/Sub con BigQuery (Streaming Directo)**  
üìå **Se puede escribir directamente en BigQuery sin usar Dataflow** con la integraci√≥n nativa de Pub/Sub.  

### **üîπ Crear una Suscripci√≥n con BigQuery como Destino**  
```bash
gcloud pubsub subscriptions create mi-sub-bq \
  --topic=mi-topico \
  --bigquery-table=mi-proyecto:dataset.tabla \
  --use-topic-schema
```

‚úî **Esto permite que Pub/Sub inserte datos directamente en BigQuery.**  
‚úî **Ideal para casos donde no se necesita transformaci√≥n en Dataflow.**  

---

## **7Ô∏è‚É£ Estrategias de Optimizaci√≥n y Mejores Pr√°cticas**  

‚úÖ **Minimizar la Latencia**  
- Usa **Dataflow con Pub/Sub** en **modo streaming** para procesamiento en tiempo real.  
- Evita el uso de suscripci√≥n **PULL** si se necesita baja latencia.  

‚úÖ **Evitar P√©rdida de Mensajes**  
- Usa **ACKs autom√°ticos** en los consumidores para confirmar la recepci√≥n.  
- Configura **reintentos** en suscripciones PUSH.  

‚úÖ **Optimizar Costos**  
- Usa **BigQuery Streaming** solo si necesitas datos en **tiempo real**.  
- Usa **Cloud Storage** como buffer antes de enviar datos a BigQuery.  

‚úÖ **Escalabilidad**  
- Configura **Auto-scaling en Dataflow** para manejar picos de carga.  
- Usa m√∫ltiples suscripciones para distribuir la carga de procesamiento.  

---

# **‚úÖ Resumen y Estrategia para Usar Pub/Sub**
| **Funcionalidad** | **Uso** |
|------------------|--------|
| **T√≥picos y Suscripciones** | Desacoplar sistemas |
| **PULL vs PUSH** | Control manual vs entrega autom√°tica |
| **Pub/Sub + Dataflow** | Procesamiento en tiempo real |
| **Pub/Sub + BigQuery** | An√°lisis de datos en tiempo real |
| **Optimizaci√≥n de costos** | Reducir uso de streaming innecesario |
